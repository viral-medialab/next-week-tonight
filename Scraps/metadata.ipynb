{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONGODB SETUP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost:27017: [Errno 61] Connection refused, Timeout: 30s, Topology Description: <TopologyDescription id: 65ab12fadac3dab5b9567ab1, topology_type: Single, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 61] Connection refused')>]>\n"
     ]
    }
   ],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"../\")\n",
    "\n",
    "uri = os.environ.get(\"MONGODB_URI\")\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select database and collection\n",
    "db = client[\"UROP\"]\n",
    "collection = db[\"News\"]\n",
    "\n",
    "# Create a document\n",
    "document = {\n",
    "    \"name\": \"John Doe\",\n",
    "    \"age\": 30,\n",
    "    \"address\": \"123 Main St\"\n",
    "}\n",
    "\n",
    "# Insert the document into the collection\n",
    "try:\n",
    "    result = collection.insert_one(document)\n",
    "    print(f\"Document added. The document ID is: {result.inserted_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BING NEWS API SETUP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Divisions emerge within Israel's war Cabinet as military scales back Gaza offensive\n",
      "URL: https://www.msn.com/en-us/news/world/divisions-emerge-within-israels-war-cabinet-as-military-scales-back-gaza-offensive/ar-AA1n8kE4\n",
      "Description: Divisions have emerged in the Israeli government over whether to prioritize eliminating Hamas or focus on negotiating the release of hostages in Gaza.\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Bing News Search API endpoint and key\n",
    "#api_endpoint = \"https://api.bing.microsoft.com/\"\n",
    "\n",
    "bing_api = os.environ.get('BING_API')\n",
    "\n",
    "api_endpoint = \"https://api.bing.microsoft.com/v7.0/news/search\"\n",
    "api_key = bing_api  # Replace with your Bing News Search API key\n",
    "query = \"Israel Gaza War\"\n",
    "# Query parameters\n",
    "query_params = {\n",
    "    \"q\": query,  # Your search query\n",
    "    \"count\": 100,  # Number of results to return (1 for the top article)\n",
    "    \"mkt\": \"en-US\"  # Market; adjust as needed\n",
    "}\n",
    "\n",
    "# Headers\n",
    "headers = {\n",
    "    \"Ocp-Apim-Subscription-Key\": api_key\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "try:\n",
    "    response = requests.get(api_endpoint, headers=headers, params=query_params)\n",
    "    response.raise_for_status()  # Raises an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "\n",
    "    # Parse the response\n",
    "    results = response.json()\n",
    "    top_article = results['value'][0] if results['value'] else None\n",
    "\n",
    "    if top_article:\n",
    "        print(f\"Title: {top_article['name']}\\nURL: {top_article['url']}\\nDescription: {top_article['description']}\")\n",
    "    else:\n",
    "        print(\"No articles found.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "articles = results['value']\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "https://www.msn.com/en-us/news/world/divisions-emerge-within-israels-war-cabinet-as-military-scales-back-gaza-offensive/ar-AA1n8kE4\n",
      "https://abcnews.go.com/International/live-updates/israel-gaza-hamas-war/idf-cant-confirm-cause-of-death-of-3-hostages-found-in-gaza-106447022?id=106373420\n",
      "https://www.msn.com/en-gb/news/world/israel-presses-assault-in-southern-gaza-jordan-says-field-hospital-badly-damaged/ar-AA1n8uAg\n",
      "https://www.nbcnews.com/news/world/live-blog/israel-hamas-war-live-updates-rcna134245?ref=biztoc.com\n",
      "https://www.msn.com/en-us/news/politics/senators-reject-bernie-sanders-effort-to-curb-israel-hamas-war-but-the-vote-signals-rising-unease/ar-AA1n51qF\n",
      "https://www.nytimes.com/video/world/middleeast/100000009269283/blinken-gaza-civilians-davos.html\n",
      "https://www.msn.com/en-us/news/politics/in-first-vote-on-gaza-war-senate-shields-israel-from-human-rights-scrutiny/ar-AA1n5E3K\n",
      "https://www.msn.com/en-us/news/politics/bernie-sanders-push-to-tie-israel-aid-to-gaza-human-rights-report-fails-as-war-rages/ar-AA1n5xgi\n",
      "https://www.msn.com/en-us/news/world/iran-fires-missiles-into-pakistan-as-israel-hamas-war-tension-spreads/ar-AA1n7J4k\n",
      "https://news.yahoo.com/un-chief-says-israel-hamas-141005299.html\n",
      "https://www.nytimes.com/2023/12/26/world/middleeast/israel-hamas-war.html\n",
      "https://www.cbsnews.com/news/israel-hamas-war-gaza-no-let-up-airstikes-christmas/\n",
      "https://www.latimes.com/world-nation/story/2023-12-22/israels-military-campaign-in-gaza-seen-as-among-deadliest-in-recent-history-experts-say\n",
      "https://www.msn.com/en-gb/news/world/fire-from-lebanon-kills-2-israeli-civilians-as-the-israel-hamas-war-rages-for-100th-day/ar-AA1mWYZa\n",
      "https://www.tbsnews.net/hamas-israel-war/israel-says-war-wind-down-southern-gaza-un-demands-ceasefire-775674\n",
      "https://www.washingtonpost.com/world/2023/12/17/israel-gaza-war-hamas-news-updates/\n",
      "https://www.npr.org/2023/12/21/1220929207/gaza-palestinians-israel-war-united-nations\n",
      "https://nypost.com/2023/12/21/news/hamas-refuses-to-hear-new-hostage-deal-as-israel-says-war-will-not-stop/\n",
      "https://www.washingtonpost.com/world/2023/12/23/israel-hamas-war-gaza-news-palestine/\n",
      "https://www.nbcnews.com/news/world/live-blog/israel-hamas-war-live-updates-rcna134245\n",
      "https://www.usnews.com/news/world/articles/2024-01-16/jordans-pm-says-peace-with-israel-remains-strategic-choice-despite-gaza-war\n",
      "https://www.msn.com/en-us/news/world/israel-in-heavy-south-gaza-strikes-as-hostages-sent-medicine/ar-AA1n79CF\n",
      "https://www.msn.com/en-gb/news/world/hostages-and-residents-in-gaza-are-set-to-receive-medicines-as-israel-hamas-war-rages-on/ar-AA1n6BQG\n",
      "https://www.usatoday.com/story/news/world/israel-hamas/2024/01/17/israel-hamas-war-gaza-live-updates/72252942007/\n",
      "https://www.barrons.com/news/israel-army-chief-says-war-with-hamas-will-last-many-more-months-c584b01b\n",
      "https://www.msn.com/en-gb/news/other/aid-for-israeli-hostages-and-palestinian-civilians-arrives-at-gaza-border/ar-AA1n7XPA\n",
      "https://www.msn.com/en-us/news/world/qatar-and-france-send-medicine-for-israeli-hostages-in-gaza-as-fighting-grinds-on/ar-AA1n7m4U\n",
      "https://nypost.com/2023/12/30/news/how-the-israel-hamas-war-in-gaza-could-go-global-in-2024/\n",
      "https://www.barrons.com/news/israel-faces-mounting-outrage-over-gaza-war-446178ae\n",
      "https://www.foxnews.com/live-news/december-19-israel-hamas-war\n",
      "https://www.msn.com/en-us/news/world/israel-under-pressure-to-scale-back-intensity-of-war-pulls-thousands-of-troops-from-gaza/ar-AA1n7slH\n",
      "https://www.msn.com/en-gb/news/other/iran-foreign-minister-in-davos-attacks-on-israel-will-end-if-gaza-war-stops/ar-AA1n7PVm\n",
      "https://www.npr.org/2023/12/19/1219748268/lebanon-hezbollah-israel-hamas-iran-war\n",
      "https://www.msn.com/en-xl/news/other/israel-and-hamas-reach-agreement-on-gaza-aid/ar-AA1n59md\n",
      "https://abcnews.go.com/International/live-updates/israel-gaza-hamas-war/?id=105899459\n",
      "https://www.msn.com/en-us/news/other/israel-s-far-right-wants-to-move-palestinians-out-of-gaza-its-ideas-are-gaining-attention/ar-AA1n6Dat\n",
      "https://www.cnn.com/middleeast/live-news/israel-hamas-war-gaza-news-01-17-24/index.html\n"
     ]
    }
   ],
   "source": [
    "def filter_msn_articles(articles):\n",
    "    return [article for article in articles if 'msn' in article['url'].lower()]\n",
    "\n",
    "\n",
    "print(len(filter_msn_articles(articles)))\n",
    "msn_articles = []\n",
    "for article in filter_msn_articles(articles):\n",
    "    msn_articles.append(article)\n",
    "\n",
    "for article in articles:\n",
    "    print(article['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "Israel presses assault in southern Gaza, Jordan says field hospital badly damaged\n",
      "\n",
      "\n",
      "\n",
      "url\n",
      "https://www.msn.com/en-gb/news/world/israel-presses-assault-in-southern-gaza-jordan-says-field-hospital-badly-damaged/ar-AA1n8uAg\n",
      "\n",
      "\n",
      "\n",
      "image\n",
      "{'thumbnail': {'contentUrl': 'https://www.bing.com/th?id=OVFT.dmY_6y-emXe0B6k4FquHBy&pid=News', 'width': 700, 'height': 473}}\n",
      "\n",
      "\n",
      "\n",
      "description\n",
      "By Arafat Barbakh, Tyrone Siu and Nidal al-Mughrabi GAZA/ISRAEL-GAZA BORDER/DOHA (Reuters) -Israel pressed its assault on Khan Younis in southern Gaza on Wednesday, sending tanks westwards and prompting accusations from Jordan that its field hospital in the city had been badly damaged by nearby shelling.\n",
      "\n",
      "\n",
      "\n",
      "provider\n",
      "[{'_type': 'Organization', 'name': 'Reuters on MSN.com', 'image': {'thumbnail': {'contentUrl': 'https://www.bing.com/th?id=ODF.jFXbg3L7Ce_1pS4_IOR8CA&pid=news'}}}]\n",
      "\n",
      "\n",
      "\n",
      "datePublished\n",
      "2024-01-17T17:21:08.0000000Z\n",
      "\n",
      "\n",
      "\n",
      "category\n",
      "World\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "\n",
    "for key in articles[i]:\n",
    "    print(key)\n",
    "    print(articles[i][key])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 403 Client Error: Forbidden for url: https://www.nytimes.com/2023/12/26/world/middleeast/israel-hamas-war.html\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        # Fetch the HTML content from the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for request errors\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Common tags where main content is found\n",
    "        content_tags = ['article', 'main', ('div', {'class': 'article-body'}), ('div', {'id': 'article-body'})]\n",
    "\n",
    "        for tag in content_tags:\n",
    "            if isinstance(tag, tuple):\n",
    "                # If tag is a tuple, it contains tag name and attributes\n",
    "                content = soup.find(tag[0], tag[1])\n",
    "            else:\n",
    "                # Otherwise, it's just a tag name\n",
    "                content = soup.find(tag)\n",
    "\n",
    "            if content:\n",
    "                return content.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        print(\"Article content not found using common tags.\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "url = articles[10]['url']\n",
    "article_text = extract_article_text(url)\n",
    "if article_text:\n",
    "    print(article_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnewspaper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Article\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# URL of the article\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43marticles\u001b[49m[\u001b[38;5;241m10\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(url)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Download and parse the article\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'articles' is not defined"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "# URL of the article\n",
    "url = articles[10]['url']\n",
    "print(url)\n",
    "# Download and parse the article\n",
    "article = Article(url)\n",
    "article.download()\n",
    "print(article.html)\n",
    "article.parse()\n",
    "\n",
    "# Access article attributes\n",
    "#print(article.authors)\n",
    "#print(article.publish_date)\n",
    "#print(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def fetch_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extracting the article content\n",
    "        content = data.get('body', 'No content found')\n",
    "        return content\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching article: {e}\")\n",
    "        return None\n",
    "\n",
    "# URL of the article\n",
    "article_url = \"https://assets.msn.com/content/view/v2/Detail/en-us/AA1mWl2n\"\n",
    "\n",
    "# Fetching the article\n",
    "article_content = fetch_article(article_url)\n",
    "for line in article_content.split(\"<p>\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "    # Find all <a> tags and decompose (remove) them\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        a_tag.decompose()\n",
    "    \n",
    "    remaining_html_content = str(soup)\n",
    "\n",
    "    soup = BeautifulSoup(remaining_html_content, 'html.parser')\n",
    "    return soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "#print(extract_text(article_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_links(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "    # Find all <a> tags and decompose (remove) them\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        a_tag.decompose()\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "# Example HTML content\n",
    "html_content = \"\"\"\n",
    "<p>This is a paragraph with <a href=\"http://example.com\">a link</a>.</p>\n",
    "<p>This is another paragraph with <a href=\"http://example.com\">another link</a>.</p>\n",
    "\"\"\"\n",
    "\n",
    "# Remove links\n",
    "cleaned_content = remove_links(html_content)\n",
    "print(cleaned_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_links_and_preserve_paragraphs(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "    # Remove all <a> tags\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        a_tag.decompose()\n",
    "\n",
    "    # Extract text from each <p> tag and concatenate with newline\n",
    "    paragraphs = [p.get_text(separator=' ', strip=True) for p in soup.find_all('p')]\n",
    "    return '\\n\\n'.join(paragraphs)\n",
    "\n",
    "\n",
    "# Example HTML content\n",
    "html_content = article_content\n",
    "\n",
    "# Clean the content\n",
    "cleaned_content = remove_links_and_preserve_paragraphs(html_content)\n",
    "print(cleaned_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example HTML content\n",
    "html_content = \"\"\"\n",
    "<p>This is a paragraph with <a href=\"http://example.com\">a link</a>.</p>\n",
    "<p>This is another paragraph with <a href=\"http://example.com\">another link</a>.</p>\n",
    "\"\"\"\n",
    "\n",
    "# Clean the content\n",
    "cleaned_content = remove_links_and_preserve_paragraphs(html_content)\n",
    "print(cleaned_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def fetch_article_contents(article):\n",
    "\n",
    "    pattern = re.compile(r'/ar-([A-Za-z0-9]+)')\n",
    "\n",
    "    article_url = article['url']\n",
    "    match = pattern.search(article_url)\n",
    "    if match:\n",
    "        article_id = match.group(1)\n",
    "    else:\n",
    "        raise Exception(\"No article ID found\")\n",
    "    \n",
    "    asset_url = \"https://assets.msn.com/content/view/v2/Detail/en-us/\" + article_id\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = requests.get(asset_url)\n",
    "        response.raise_for_status()  # Ensure the request was successful\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extracting the article content\n",
    "        html_content = data.get('body', 'No content found')\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching article: {e}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "    # Remove all <a> tags\n",
    "    #for a_tag in soup.find_all('a'):\n",
    "    #    a_tag.decompose()\n",
    "\n",
    "    # Extract text from each <p> tag and concatenate with newline\n",
    "    paragraphs = [p.get_text(separator=' ', strip=True) for p in soup.find_all('p')]\n",
    "    return '\\n\\n'.join(paragraphs)\n",
    "\n",
    "# URL of the article\n",
    "#article_url = \"https://assets.msn.com/content/view/v2/Detail/en-us/AA1mY0HM\"\n",
    "#print(fetch_article_contents(article_url))\n",
    "#print(msn_articles[1]['url'])\n",
    "#print(fetch_article_contents(msn_articles[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO LIST:\n",
    "\n",
    "    - Figure out how to extract article id from article URL for MSN articles\n",
    "    - Extract author similarly to how we extracted article contents\n",
    "    - Extract tone and sentiment using GPT4 API and our article contents.\n",
    "    - Extract semantic embeddings for articles\n",
    "    - Create a test script to see whether the metadata is roughly correct for a handful of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_article_ids(urls):\n",
    "    article_ids = []\n",
    "    pattern = re.compile(r'/ar-([A-Za-z0-9]+)')\n",
    "\n",
    "    for url in urls:\n",
    "        match = pattern.search(url)\n",
    "        if match:\n",
    "            article_ids.append(match.group(1))\n",
    "\n",
    "    return article_ids\n",
    "\n",
    "# List of URLs\n",
    "urls = [\n",
    "    \"https://www.msn.com/en-us/news/world/israel-gaza-live-updates-war-between-israel-and-hamas-reaches-the-100-day-mark/ar-AA1mBEvj\",\n",
    "    \"https://www.msn.com/en-gb/news/world/a-defiant-netanyahu-says-no-one-can-halt-israels-war-to-crush-hamas-including-the-world-court/ar-AA1mUhLL\",\n",
    "    # ... other URLs\n",
    "]\n",
    "\n",
    "# Extract article IDs\n",
    "article_ids = extract_article_ids(urls)\n",
    "print(article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for article in articles:\n",
    "    if 'msn' in article['url']:\n",
    "        metadata = {}\n",
    "        metadata['url'] = article['url']\n",
    "        metadata['date_published'] = article['datePublished'][:19].replace('T', ' ')\n",
    "        metadata['publisher'] = article['provider'][0]['name'][:-11]\n",
    "        metadata['category'] = article['category']\n",
    "        metadata['keywords'] = [d['name'] for d in article['about']]\n",
    "        for key in metadata:\n",
    "            print(key, \":\", metadata[key])\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "def chat_with_gpt(prompt):\n",
    "    openai.api_key = os.environ.get(\"OPENAI_API\")\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # or the specific chat model you are using\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message['content']\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Example usage \n",
    "for article in articles:\n",
    "    if 'msn' in article['url']:\n",
    "        try:\n",
    "            prompt = \"On a scale of 0 to 100, where 0 is overly negative and 100 is overly positive, what is the sentiment of this article as a whole? Please only return the number with no further explanation. Please imagine you must be consistent and you get punished for two different instances of you providing two different answers.\" + fetch_article_contents(article)\n",
    "            response = chat_with_gpt(prompt)\n",
    "            print(response)\n",
    "        except:\n",
    "            print(article['url'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run 1:\n",
    "\n",
    "50\n",
    "25\n",
    "20\n",
    "30\n",
    "50\n",
    "Error fetching article: 410 Client Error: Gone for url: https://assets.msn.com/content/view/v2/Detail/en-us/AA1mWmtw\n",
    "https://www.msn.com/en-gb/news/world/middle-east-crisis-live-netanyahu-says-world-court-cannot-stop-israel-in-gaza-as-war-enters-100th-day/ar-AA1mWmtw\n",
    "33\n",
    "10\n",
    "\n",
    "run 2:\n",
    "\n",
    "40\n",
    "20\n",
    "26\n",
    "50\n",
    "50\n",
    "Error fetching article: 410 Client Error: Gone for url: https://assets.msn.com/content/view/v2/Detail/en-us/AA1mWmtw\n",
    "https://www.msn.com/en-gb/news/world/middle-east-crisis-live-netanyahu-says-world-court-cannot-stop-israel-in-gaza-as-war-enters-100th-day/ar-AA1mWmtw\n",
    "30\n",
    "0\n",
    "\n",
    "run 3:\n",
    "\n",
    "50\n",
    "15\n",
    "30\n",
    "57\n",
    "50\n",
    "Error fetching article: 410 Client Error: Gone for url: https://assets.msn.com/content/view/v2/Detail/en-us/AA1mWmtw\n",
    "https://www.msn.com/en-gb/news/world/middle-east-crisis-live-netanyahu-says-world-court-cannot-stop-israel-in-gaza-as-war-enters-100th-day/ar-AA1mWmtw\n",
    "20\n",
    "20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fetch_article_contents(msn_articles[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Encode text\n",
    "input_text = \"Your text here\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The last hidden state is the sequence of hidden states of the last layer\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print(last_hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDINGS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"A defiant Netanyahu says no one can halt Israel's war to crush Hamas, including the world court\",\n",
    "    \"Rwanda bill – latest: Boris Johnson and Tory deputy chair back rebellion as Rishi Sunak faces crunch vote\",\n",
    "    \"I’m obsessed with Korean drama – and now it’s inspired my perfect midlife escape\",\n",
    "    \"The 20 Most Ignored Cancer Symptoms in Women and Men\"\n",
    "]\n",
    "\n",
    "# Step 1: Tokenize the documents\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "\n",
    "# Step 2: Train Word2Vec model\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a weighted document vector\n",
    "def create_weighted_embedding(doc):\n",
    "    doc_tokens = doc.lower().split()\n",
    "    weighted_embedding = np.zeros(model.vector_size)  # Use the actual vector size of the model\n",
    "    count = 0\n",
    "    for word in doc_tokens:\n",
    "        if word in model and word in feature_names:\n",
    "            #tfidf_score = tfidf_matrix[documents.index(doc), feature_names.tolist().index(word)]\n",
    "            weighted_embedding += model[word] #* tfidf_score  # Directly use model[word]\n",
    "            count += 1\n",
    "    return weighted_embedding / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m documents \u001b[38;5;241m=\u001b[39m [article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Step 3: Calculate TF-IDF scores\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mTfidfVectorizer\u001b[49m()\n\u001b[1;32m     26\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(documents)\n\u001b[1;32m     27\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to print similarity scores between document pairs\n",
    "def print_similarity_scores(embeddings, x = None, y = None):\n",
    "    max_sim_score = -1.1\n",
    "    vals = None\n",
    "    if x == None and y == None:\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(i+1, len(embeddings)):\n",
    "                sim_score = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "                if sim_score >= max_sim_score and sim_score < .9:\n",
    "                    vals = (i, j)\n",
    "                    max_sim_score = sim_score\n",
    "                #print(f\"Similarity between Document {i+1} and Document {j+1}: {sim_score:.4f}\")\n",
    "        print(vals)\n",
    "        sim_score = cosine_similarity([embeddings[vals[0]]], [embeddings[vals[1]]])[0][0]\n",
    "        print(f\"Similarity between Document {vals[0]} and Document {vals[1]}: {sim_score:.4f}\")\n",
    "    else:\n",
    "        sim_score = cosine_similarity([embeddings[x]], [embeddings[y]])[0][0]\n",
    "        print(f\"Similarity between Document {x} and Document {y}: {sim_score:.4f}\")\n",
    "\n",
    "documents = [article['description'] for article in articles]\n",
    "\n",
    "# Step 3: Calculate TF-IDF scores\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 4: Compute weighted embeddings for each document\n",
    "weighted_doc_embeddings = [create_weighted_embedding(doc) for doc in documents]\n",
    "\n",
    "# Print similarity scores\n",
    "def print_stuff(x, y):\n",
    "    # Print similarity scores\n",
    "    print_similarity_scores(weighted_doc_embeddings, x, y)\n",
    "    print(\"article {i} description: \".format(i=x), articles[x]['description'])\n",
    "    print(\"article {j} description: \".format(j=y), articles[y]['description'])\n",
    "\n",
    "'''\n",
    "a = 1\n",
    "b = 7\n",
    "c = 9\n",
    "print_stuff(a,b)\n",
    "print_stuff(a,c)\n",
    "print_stuff(b,c)\n",
    "'''\n",
    "print_similarity_scores(weighted_doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "b = 11\n",
    "c = 31\n",
    "d = 32\n",
    "print_stuff(a,b)\n",
    "print_stuff(a,c)\n",
    "print_stuff(b,c)\n",
    "print_stuff(a,d)\n",
    "print_stuff(b,d)\n",
    "print_stuff(c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "print(cosine_similarity([model['negative']], [model['positive']])[0][0])\n",
    "print(scipy.spatial.distance.cosine(model['negative'],model['positive']))\n",
    "print(scipy.spatial.distance.euclidean(model['negative'],model['positive']))\n",
    "print(scipy.spatial.distance.euclidean(model['negative'],model['negativity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "glove_vectors = gensim.downloader.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('Gaza')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_similarity([model['twitter']], [model['Twitter']])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "c=0\n",
    "# Example article\n",
    "for article in articles:\n",
    "    try:\n",
    "        # Create a TextBlob object\n",
    "        blob = TextBlob(fetch_article_contents(article))\n",
    "\n",
    "        # Get sentiment\n",
    "        sentiment = blob.sentiment\n",
    "\n",
    "        print(f\"Article {c} contents - Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\")\n",
    "\n",
    "        # Create a TextBlob object\n",
    "        print(article['description'])\n",
    "        blob = TextBlob(article['description'])\n",
    "\n",
    "        # Get sentiment\n",
    "        sentiment = blob.sentiment\n",
    "\n",
    "        print(f\"Article {c} description - Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\")\n",
    "        c+= 1\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Example article\n",
    "article = \"Person saves puppies and many find that act good.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(article)\n",
    "\n",
    "# Get sentiment\n",
    "sentiment = blob.sentiment\n",
    "print(sentiment)\n",
    "\n",
    "print(f\"Polarity: {sentiment.polarity}, Subjectivity: {sentiment.subjectivity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained model tokenizer and model (TinyBERT or MobileBERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-tiny')  # Example for TinyBERT\n",
    "model = AutoModelForSequenceClassification.from_pretrained('prajjwal1/bert-tiny')\n",
    "\n",
    "# Create a pipeline for text classification\n",
    "nlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example text\n",
    "text = \"Replace this with the text you want to classify.\"\n",
    "\n",
    "# Predict the class of the text\n",
    "result = nlp(text)\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "text = \"Replace this with the text you want to classify.\"\n",
    "\n",
    "# Predict the class of the text\n",
    "result = nlp(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSSIBLE USE OF FEEDLY ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedly is very expensive but seems like it could be very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_feedly_articles(api_key, stream_id, count=10):\n",
    "    \"\"\"\n",
    "    Fetches articles from a specified Feedly stream.\n",
    "\n",
    "    :param api_key: Your Feedly API key.\n",
    "    :param stream_id: The ID of the Feedly stream (e.g., a feed or category).\n",
    "    :param count: The number of articles to fetch.\n",
    "    :return: A list of articles.\n",
    "    \"\"\"\n",
    "    url = f\"https://cloud.feedly.com/v3/streams/contents?streamId={stream_id}&count={count}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        articles = data.get(\"items\", [])\n",
    "        return [{\"title\": article[\"title\"], \"link\": article[\"originId\"]} for article in articles]\n",
    "    else:\n",
    "        print(\"Failed to fetch articles\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "api_key = # Replace with your actual API key\n",
    "stream_id = \"stream/your_stream_id\"  # Replace with the actual stream ID for Israel-Gaza war articles\n",
    "articles = get_feedly_articles(api_key, stream_id)\n",
    "for article in articles:\n",
    "    print(f\"Title: {article['title']}, Link: {article['link']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING ROBERTA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcardiffnlp/twitter-xlm-roberta-base-sentiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcardiffnlp/twitter-xlm-roberta-base-sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_fast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/news/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:745\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/news/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1854\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1852\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/news/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2017\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2017\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2021\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2022\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/news/lib/python3.10/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:155\u001b[0m, in \u001b[0;36mXLMRobertaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    141\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m ):\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     mask_token \u001b[38;5;241m=\u001b[39m AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_save_slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/news/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", use_fast = False)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Interpret the result\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    return model.config.id2label[predicted_class_id]\n",
    "\n",
    "# Example usage\n",
    "article_text = \"Your article text goes here\"\n",
    "sentiment = analyze_sentiment(article_text)\n",
    "print(\"Sentiment:\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING OPENAI EMBEDDINGS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Document 0 and Document 1: 0.8652\n",
      "Similarity between Document 0 and Document 2: 0.7818\n",
      "Similarity between Document 1 and Document 2: 0.7484\n"
     ]
    }
   ],
   "source": [
    "engine = 'text-embedding-ada-002'\n",
    "\n",
    "import openai\n",
    "\n",
    "def get_embedding(text, engine=engine):\n",
    "    \"\"\"\n",
    "    Get the embedding for the given text using OpenAI's Embedding API.\n",
    "\n",
    "    :param text: The text to embed.\n",
    "    :param engine: The embedding engine to use.\n",
    "    :return: Embedding vector.\n",
    "    \"\"\"\n",
    "    openai.api_key = os.environ.get(\"OPENAI_API\")  # Replace with your OpenAI API key\n",
    "\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        engine=engine\n",
    "    )\n",
    "\n",
    "    embedding = response[\"data\"][0][\"embedding\"]\n",
    "    return embedding\n",
    "\n",
    "# Example usage\n",
    "article_text = fetch_article_contents(msn_articles[4])\n",
    "embedding_1 = get_embedding(article_text)\n",
    "\n",
    "article_text = fetch_article_contents(msn_articles[5])\n",
    "embedding_2 = get_embedding(article_text)\n",
    "\n",
    "article_text = fetch_article_contents(msn_articles[6])\n",
    "article_text= \"Vice President Kamala Harris was asked on “The View” this morning why the Biden-Harris campaign continues to focus on the threat Donald Trump poses to democracy. One host, Sara Haines, said that doesn’t seem to be “moving the needle” and wondered if the campaign should focus more on issues like the economy and immigration. Harris replied that such issues “are inextricably linked” with the health of the country’s democracy.\"\n",
    "embedding_3 = get_embedding(article_text)\n",
    "\n",
    "embeddings = [embedding_1, embedding_2, embedding_3]\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to print similarity scores between document pairs\n",
    "def print_similarity_scores(embeddings, x, y):\n",
    "    sim_score = cosine_similarity([embeddings[x]], [embeddings[y]])[0][0]\n",
    "    print(f\"Similarity between Document {x} and Document {y}: {sim_score:.4f}\")\n",
    "\n",
    "# Print similarity scores\n",
    "def print_stuff(x, y):\n",
    "    # Print similarity scores\n",
    "    print_similarity_scores(embeddings, x, y)\n",
    "\n",
    "print_stuff(0,1)\n",
    "print_stuff(0,2)\n",
    "print_stuff(1,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JERUSALEM (AP) — The White House said Sunday that “it’s the right time” for Israel to scale back its military offensive in the Gaza Strip, as Israeli leaders again vowed to press ahead with their operation against the territory’s ruling Hamas militant group.\n",
      "\n",
      "The comments exposed the growing differences between the close allies on the 100th day of the war.\n",
      "\n",
      "Also Sunday, Israeli warplanes struck targets in Lebanon following a Hezbollah missile attack that killed two Israeli civilians — an older woman and her adult son — in northern Israel. The exchange of fire underscored concerns that the Gaza violence could trigger wider fighting across the region.\n",
      "\n",
      "The war in Gaza, launched by Israel in response to the unprecedented Oct. 7 attack by Hamas, has killed nearly 24,000 Palestinians, devastated vast swaths of Gaza, driven around 85% of the territory’s 2.3 million residents from their homes and pushed a quarter of the population into starvation .\n",
      "\n",
      "Speaking on CBS, White House National Security Council spokesman John Kirby said the U.S. has been speaking to Israel “about a transition to low-intensity operations” in Gaza.\n",
      "\n",
      "“We believe it’s the right time for that transition. And we’re talking to them about doing that,” he said on “Face the Nation.”\n",
      "\n",
      "Israel launched the offensive after the Hamas attack killed some 1,200 people, mostly civilians, and took 250 others hostage. Prime Minister Benjamin Netanyahu has vowed to press ahead until Hamas is destroyed and all of the more than 100 hostages still in captivity are freed.\n",
      "\n",
      "The war has sent tensions soaring across the region , with Israel trading fire almost daily with Lebanon’s Hezbollah militant group and Iranian-backed militias attacking U.S. targets in Syria and Iraq. In addition, Yemen’s Houthi rebels have been targeting international shipping, drawing a wave of U.S. airstrikes last week.\n",
      "\n",
      "Hezbollah’s leader, Hassan Nasrallah, said his group won’t stop until a cease-fire is in place for Gaza.\n",
      "\n",
      "“We are continuing, and our front is inflicting losses on the enemy and putting pressure on displaced people,” Nasrallah said in a speech, referring to the tens of thousands of Israelis who have fled northern border areas.\n",
      "\n",
      "In other developments, tens of thousands of people in Europe and the Middle East took to the streets Sunday to mark the 100th day of the war. Opposing demonstrations either demanded the release of Israeli hostages held by Hamas or called for a cease-fire in Gaza.\n",
      "\n",
      "In Israel, supporters of the hostages and their families wrapped up a 24-hour protest in Tel Aviv calling on the government to win their immediate release.\n",
      "\n",
      "Late Sunday, Hamas released a short video in which three Israeli hostages, presumably speaking under duress, pleaded with their government to end the war and bring them home. It was not clear when the video was taken. At the end of the clip, Hamas said it would provide an update on their fate Monday.\n",
      "\n",
      "Hamas has released several videos of this kind, in an apparent effort to pressure Israel to agree to its demand of ending the war ahead of negotiations on a possible release of all hostages in exchange for most or all Palestinian prisoners held by Israel.\n",
      "\n",
      "The unprecedented level of death and destruction in Gaza has led South Africa to lodge allegations of genocide against Israel at the International Court of Justice. Israel denies the accusations and has vowed to press ahead with its offensive even if the court in The Hague issues an interim order for it to stop.\n",
      "\n",
      "Israel has also been under growing international pressure to end the war in Gaza, but it has so far been shielded by U.S. diplomatic and military support. Israel argues that any cease-fire would hand victory to Hamas, which has ruled Gaza since 2007 and is bent on Israel’s destruction.\n",
      "\n",
      "“It’s been 100 days, yet we will not stop until we win,” Defense Minister Yoav Gallant said Sunday.\n",
      "\n",
      "But differences with the Americans have begun to emerge. During a visit to the region last week, Secretary of State Antony Blinken renewed his calls on Israel to do more to reduce civilian casualties and increase the supplies of desperately needed humanitarian aid entering Gaza.\n",
      "\n",
      "In recent weeks, Israel has scaled back operations in northern Gaza, the initial target of the offensive, where weeks of airstrikes and ground operations left entire neighborhoods in ruins .\n",
      "\n",
      "Kirby, the White House spokesman, acknowledged that Israel had taken some “precursory steps” toward scaling back the offensive. But he said there was more to do.\n",
      "\n",
      "“We’re not saying let your foot up off the gas completely and don’t keep going after Hamas,” he said. “It’s just that we believe the time is coming here very, very soon for a transition to this lower intensity phase.”\n",
      "\n",
      "The deadly Hezbollah missile strike in northern Israel renewed concerns about a second front erupting into full-blown war.\n",
      "\n",
      "It came shortly after the Israeli army said it killed three Lebanese militants who tried to infiltrate Israel.\n",
      "\n",
      "Late Sunday, the Israeli military said it had struck Lebanon in response to the missile strike. Israeli officials said a woman in her 70s and her son, in his 40s, were killed in the town of Yuval.\n",
      "\n",
      "The army’s chief spokesman, Rear Adm. Daniel Hagari, said Israel would not tolerate attacks on civilians.\n",
      "\n",
      "“The price will be extracted not just tonight, but also in the future,” he said.\n",
      "\n",
      "Yuval is one of more than 40 towns along Israel’s northern border evacuated by the government in October. Israeli media reported that the family stayed in the area because they work in agriculture.\n",
      "\n",
      "Tensions have also spread to the Israeli-occupied West Bank, where Palestinian health officials say nearly 350 Palestinians have been killed by Israeli fire in confrontations throughout the war.\n",
      "\n",
      "On Sunday, the Israeli army said troops opened fire after a Palestinian car breached a military roadblock in the southern West Bank and an attacker fired at soldiers. Palestinian health officials said two Palestinians were killed.\n",
      "\n",
      "Late Sunday, Palestinian health officials said two teenage boys were killed by Israeli fire. The army said it shot them after they threw a bomb at an army base.\n",
      "\n",
      "Israel has launched major operations against the southern city of Khan Younis and built-up refugee camps in central Gaza.\n",
      "\n",
      "“No one is able to move,” said Rami Abu Matouq, who lives in the Maghazi camp. “Warplanes, snipers and gunfire are everywhere.”\n",
      "\n",
      "In the central town of Deir al-Balah, health officials said at least 15 people were killed in Israeli strikes late Saturday.\n",
      "\n",
      "At the entrance of the Al-Aqsa Martyrs Hospital, men lined up to pray for the dead, their bodies wrapped in white shrouds. The bodies were put on the back of a pickup truck before they were taken to be buried.\n",
      "\n",
      "Meanwhile, the Egyptian TV station Al-Ghad said a cameraman was killed in an Israeli airstrike in northern Gaza. The channel said Yazan al-Zwaidi was apparently in a crowd of people at the time. Details were not immediately available, and the Israeli military had no comment.\n",
      "\n",
      "The internet advocacy group Netblocks said communications in Gaza were still out after a 48-hour outage. The Palestinian telecommunications operator in Gaza, Jawwal, said two of its employees were killed Saturday when they were hit by a shell while fixing lines in Khan Younis.\n",
      "\n",
      "The Gaza Health Ministry said Sunday that hospitals had received 125 bodies in the last 24 hours, bringing the overall death toll to 23,968. The ministry does not differentiate between civilians and combatants but says around two-thirds of the dead are women and minors. It says over 60,000 people have been wounded.\n",
      "\n",
      "Israel says Hamas is responsible for the high civilian casualties, saying its fighters make use of civilian buildings and launch attacks from densely populated urban areas. The military says 189 soldiers have been killed and 1,099 wounded since the start of the ground offensive.\n",
      "\n",
      "___\n",
      "\n",
      "Magdy reported from Cairo. Chehayeb reported from Beirut. Associated Press writer Abby Sewell in Beirut contributed to this report.\n",
      "\n",
      "___\n",
      "\n",
      "Find more of AP’s coverage at https://apnews.com/hub/israel-hamas-war\n"
     ]
    }
   ],
   "source": [
    "print(fetch_article_contents(msn_articles[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'U.S. Pressed Israel to Reduce Civilian Suffering in Gaza, Blinken Says', 'url': 'https://www.nytimes.com/video/world/middleeast/100000009269283/blinken-gaza-civilians-davos.html', 'image': {'thumbnail': {'contentUrl': 'https://www.bing.com/th?id=OVFT.1B8pSxVDtEj5ZGv30kxZ-y&pid=News', 'width': 700, 'height': 366}}, 'description': 'Secretary of State Antony J. Blinken described the war’s toll on civilians in Gaza as “gut-wrenching” at the World Economic Forum in Davos, Switzerland.', 'about': [{'readLink': 'https://api.bing.microsoft.com/api/v7/entities/1ffafed3-2b37-b871-c271-aa855d98449a', 'name': 'Israel'}, {'readLink': 'https://api.bing.microsoft.com/api/v7/entities/90eb9ff6-60ad-da43-d3fe-1ee3e3cb775a', 'name': 'Gaza Strip'}, {'readLink': 'https://api.bing.microsoft.com/api/v7/entities/e905a78a-0b3e-16d8-aae8-ba61ab76b847', 'name': 'Pressed'}], 'provider': [{'_type': 'Organization', 'name': 'The New York Times', 'image': {'thumbnail': {'contentUrl': 'https://www.bing.com/th?id=ODF.GzcmUDr41J6Qc1JEQyNTCA&pid=news'}}}], 'datePublished': '2024-01-17T15:03:00.0000000Z'}\n"
     ]
    }
   ],
   "source": [
    "print(articles[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
