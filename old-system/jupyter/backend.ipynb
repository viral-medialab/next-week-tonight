{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Liquid News Backend V.1.0\n",
        "\n",
        "Liquid News aims to help people better understand and interact with the news by providing a machine-learning-based analysis and semantic navigational aids. This will allow users to parse news via a semantic-relational model that leverages the latent connection between news segments. The hope is that this will uncover the relationships between topics covered across multiple news sources and promote a greater understanding of the news and media around us."
      ],
      "metadata": {
        "id": "XdydkoXnOwFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Drive & Imports\n",
        "\n",
        "This module will connect to your google drive, which this collab will use for storage in V1.0 of Liquid News. It will create a ```LiquidNews``` directory in the parent directory of your drive."
      ],
      "metadata": {
        "id": "ESIsBVDFO1oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect To Your Drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/LiquidNews'\n",
        "if not os.path.isdir(path):\n",
        "  os.mkdir(path)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxfBYeSWPuUU",
        "outputId": "0980e3aa-bf8d-4549-8649-886865cd66e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILsKJZ3kPZDl"
      },
      "outputs": [],
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install pytube\n",
        "! pip install openai\n",
        "! pip install pysrt\n",
        "! pip install --upgrade youtube_dl\n",
        "! pip install vtt_to_srt3\n",
        "! pip install pysrt\n",
        "! pip install openai\n",
        "! pip install moviepy\n",
        "! pip install imageio==2.4.1\n",
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "import pysrt\n",
        "import youtube_dl\n",
        "from datetime import datetime\n",
        "import vtt_to_srt.vtt_to_srt\n",
        "import pandas as pd\n",
        "from os.path import exists\n",
        "from random import randint\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "import os\n",
        "import fnmatch\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import re\n",
        "import whisper\n",
        "import pickle\n",
        "import uuid"
      ],
      "metadata": {
        "id": "j_JmLwEaQW7v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Whisper Transcription Model\n",
        "\n",
        "The ```whisper_model``` variable is the OpenAI whisper model used to transcribe an audio file to text"
      ],
      "metadata": {
        "id": "NKyRTqdtPbji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_model = whisper.load_model(\"large\")"
      ],
      "metadata": {
        "id": "sz9ZcpKakgcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f383ed-67a9-40a7-cd2d-4bbfb7b50f25"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.87G/2.87G [01:12<00:00, 42.8MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Download and Information Retrieval Module\n",
        "\n",
        "The following three methods are used to download a video from Youtube, download the audio file for a respective Youtube video, and get the metadata associated with the video (i.e., name, video_id)"
      ],
      "metadata": {
        "id": "8ngz5E9qQCl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_information(video_link):\n",
        "  title, id = None, None\n",
        "  ydl_opts_info = {\"skip_download\": True}\n",
        "  with youtube_dl.YoutubeDL(ydl_opts_info) as ydl:\n",
        "    title = ydl.extract_info(video_link, download=False).get('title',None)\n",
        "    id = ydl.extract_info(video_link, download=False).get('id',None)\n",
        "  return title, id\n",
        "\n",
        "def download_video(video_link, output_path, skip_download=False):\n",
        "  ydl_opts = {\n",
        "      \"writesubtitles\": True, \n",
        "      \"writeautomaticsub\": True, \n",
        "      \"skip_download\": skip_download, \n",
        "      \"subtitleslangs\":[\"en\"], \n",
        "      \"outtmpl\": f\"{output_path}/%(id)s.%(ext)s\"\n",
        "  }\n",
        "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "      ydl.download([video_link])\n",
        "\n",
        "def download_video_audio(video_link, output_path, skip_download=False):\n",
        "  ydl_opts = {\n",
        "    'format': 'bestaudio/best',\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'mp3',\n",
        "        'preferredquality': '192',\n",
        "    }],\n",
        "    \"outtmpl\": f\"{output_path}/%(id)s.%(ext)s\"\n",
        "  }\n",
        "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "      ydl.download([video_link])"
      ],
      "metadata": {
        "id": "HoZkxBYYR3XF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcription Module\n",
        "\n",
        "The following methods leverage the ```whisper_model``` and the audio file associated with a video to generate audio transcription. The transcription is then converted into an SRT format used for video captioning."
      ],
      "metadata": {
        "id": "6CesMEyyRAxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_timestamp(seconds: float, always_include_hours: bool = False, fractionalSeperator: str = '.'):\n",
        "    assert seconds >= 0, \"non-negative timestamp expected\"\n",
        "    milliseconds = round(seconds * 1000.0)\n",
        "\n",
        "    hours = milliseconds // 3_600_000\n",
        "    milliseconds -= hours * 3_600_000\n",
        "\n",
        "    minutes = milliseconds // 60_000\n",
        "    milliseconds -= minutes * 60_000\n",
        "\n",
        "    seconds = milliseconds // 1_000\n",
        "    milliseconds -= seconds * 1_000\n",
        "\n",
        "    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n",
        "    return f\"{hours_marker}{minutes:02d}:{seconds:02d}{fractionalSeperator}{milliseconds:03d}\"\n",
        "\n",
        "def write_srt(transcript, file):\n",
        "    \"\"\"\n",
        "    Write a transcript to a file in SRT format.\n",
        "    Example usage:\n",
        "        from pathlib import Path\n",
        "        from whisper.utils import write_srt\n",
        "        result = transcribe(model, audio_path, temperature=temperature, **args)\n",
        "        # save SRT\n",
        "        audio_basename = Path(audio_path).stem\n",
        "        with open(Path(output_dir) / (audio_basename + \".srt\"), \"w\", encoding=\"utf-8\") as srt:\n",
        "            write_srt(result[\"segments\"], file=srt)\n",
        "    \"\"\"\n",
        "    for i, segment in enumerate(transcript, start=1):\n",
        "        # write srt lines\n",
        "        print(\n",
        "            f\"{i}\\n\"\n",
        "            f\"{format_timestamp(segment['start'], always_include_hours=True, fractionalSeperator=',')} --> \"\n",
        "            f\"{format_timestamp(segment['end'], always_include_hours=True, fractionalSeperator=',')}\\n\"\n",
        "            f\"{segment['text'].strip().replace('-->', '->')}\\n\",\n",
        "            file=file,\n",
        "            flush=True,\n",
        "        )\n",
        "\n",
        "def audio_to_caption(audio_path, output_path, whisper_model):\n",
        "  video_transcription = whisper.transcribe(\n",
        "    whisper_model,\n",
        "    str(audio_path),\n",
        "    temperature=0.9,\n",
        "    verbose=False\n",
        "    )\n",
        "  result = whisper_model.transcribe(audio_path)\n",
        "  return result"
      ],
      "metadata": {
        "id": "xZ090iAuasEM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcription Tokenizer\n",
        "\n",
        "The following ```caption_tokenizer``` method breaks down the transcription for a given video into a token based on sentence length. The user inputs the ```num_sentences_per_token``` parameter to determine the token size used for embedding"
      ],
      "metadata": {
        "id": "xOn5WR6nRUxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def caption_tokenizer(segments, num_sentences_per_token, id, liquid_videos_data):\n",
        "  index = 0\n",
        "  token = \"\"\n",
        "  sentence_count = num_sentences_per_token\n",
        "  start = segments[0]['start']\n",
        "  while index < len(segments)-1:\n",
        "    if any(punctuation in segments[index]['text'] for punctuation in [\".\",\"?\",\"!\"]) and sentence_count == 1:\n",
        "      token += segments[index]['text']\n",
        "      key = (start, segments[index]['end'])\n",
        "      liquid_videos_data[id]['caption_token_data'][key] = {\"text\": token}\n",
        "      sentence_count = num_sentences_per_token\n",
        "      start = segments[index+1]['start']\n",
        "      token = \"\"\n",
        "    elif any(punctuation in segments[index]['text'] for punctuation in [\".\",\"?\",\"!\"]) and sentence_count > 1:\n",
        "      token += segments[index]['text']\n",
        "      sentence_count -= 1\n",
        "    else:\n",
        "      token += segments[index]['text']\n",
        "    index+=1\n",
        "    \n",
        "  token += segments[index]['text']\n",
        "  key = (start, segments[index]['end'])\n",
        "  liquid_videos_data[id]['caption_token_data'][key] = {\"text\": token}"
      ],
      "metadata": {
        "id": "ouEMYuYvAfyG"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Module\n",
        "\n",
        "The following methods are used to get the embeddings for each token using the GPT-3 ```text-similarity-davinci-001``` model. "
      ],
      "metadata": {
        "id": "j1A_bGHeRyIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "openai.api_key = \"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\"\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embedding(text: str, engine=\"text-similarity-davinci-001\"):\n",
        "    return openai.Embedding.create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
        "def get_token_embedding(liquid_videos_data, id, num_sentences_per_token, directory_path):\n",
        "  embeddings = []\n",
        "  # if exists(f\"{directory_path}/embedding_tokensize_{num_sentences_per_token}/{id}_embedding.text\"):\n",
        "  #   embeddings = np.loadtxt(f\"{directory_path}/embedding_tokensize_{num_sentences_per_token}/{id}_embedding.text\")\n",
        "  # else:\n",
        "  for key in liquid_videos_data[id]['caption_token_data']:\n",
        "    embedding = get_embedding(liquid_videos_data[id]['caption_token_data'][key]['text'])\n",
        "    liquid_videos_data[id]['caption_token_data'][key]['embedding'] = embedding\n",
        "  os.makedirs(os.path.dirname(f\"{directory_path}/embedding_tokensize_{num_sentences_per_token}/{id}_embedding.text\"), exist_ok=True)\n",
        "  np.savetxt(f\"{directory_path}/embedding_tokensize_{num_sentences_per_token}/{id}_embedding.text\", embeddings)\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "W-e3Y5PSbKKf"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering Module\n",
        "\n",
        "The following methods are used to cluster the tokens and then determine a topic for each cluster."
      ],
      "metadata": {
        "id": "urWwBAyfSGtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clusterings(liquid_videos_data, id,  n_clusters):\n",
        "  keys = list(liquid_videos_data[id]['caption_token_data'].keys())\n",
        "  keys.sort()\n",
        "  embeddings = [liquid_videos_data[id]['caption_token_data'][key]['embedding'] for key in keys]\n",
        "  kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42)\n",
        "  kmeans.fit(embeddings)\n",
        "  labels = kmeans.labels_\n",
        "  for i in range(len(labels)):\n",
        "    liquid_videos_data[id]['caption_token_data'][keys[i]]['cluster'] = labels[i]"
      ],
      "metadata": {
        "id": "JL1Vb7r_dkJ_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cluster_topic(liquid_videos_data, id):\n",
        "  cluster_data = {}\n",
        "  for key in liquid_videos_data[id]['caption_token_data']:\n",
        "    if liquid_videos_data[id]['caption_token_data'][key]['cluster'] in cluster_data:\n",
        "        cluster_data[liquid_videos_data[id]['caption_token_data'][key]['cluster']]['summary'] += liquid_videos_data[id]['caption_token_data'][key]['text']\n",
        "    else:\n",
        "        cluster_data[liquid_videos_data[id]['caption_token_data'][key]['cluster']] = {'summary': liquid_videos_data[id]['caption_token_data'][key]['text']}\n",
        "  for cluster in cluster_data:\n",
        "    summary = cluster_data[cluster]['summary']\n",
        "    prompt = f\"key idea of this paragraph in three words. \\n {summary}\"\n",
        "    key_topic = openai.Completion.create(engine=\"text-davinci-002\", prompt=prompt, temperature=0.7, max_tokens=32,top_p=0.9)\n",
        "    key_topic = key_topic[\"choices\"][0][\"text\"]\n",
        "    key_topic = key_topic.replace(\" \", \"_\")\n",
        "    summary = cluster_data[cluster]['topic'] = key_topic\n",
        "  return cluster_data"
      ],
      "metadata": {
        "id": "6DJ-YywAi07I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Segmentation Module\n",
        "\n",
        "The following module uses the cluster information from the cluster module to generate video clips for each cluster using the algorithm defined in the ```reduce_cluster_clip_windows``` method."
      ],
      "metadata": {
        "id": "a1nWH4CRS0Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_cluster_clip_windows(windows):\n",
        "  clips = []\n",
        "  start = windows[0][0]\n",
        "  end = windows[0][1]\n",
        "  for i in range(len(windows)):\n",
        "    if windows[i][0] - end > 30:\n",
        "      clips.append((start,end))\n",
        "      start = windows[i][0]\n",
        "      end = windows[i][1]\n",
        "      if i == len(windows)-1:\n",
        "        clips.append(windows[i])\n",
        "    else:\n",
        "      end = windows[i][1]\n",
        "      if i == len(windows)-1:\n",
        "          clips.append((start,end))\n",
        "  return clips\n",
        "\n",
        "def get_cluster_clips_windows(liquid_videos_data, id):\n",
        "  clip_windows = sorted(list(liquid_videos_data[id]['caption_token_data'].keys()))\n",
        "  for window in clip_windows:\n",
        "    window_cluster = liquid_videos_data[id]['caption_token_data'][window]['cluster']\n",
        "    if 'clip_windows' in  liquid_videos_data[id]['cluster_data'][window_cluster]:\n",
        "      liquid_videos_data[id]['cluster_data'][window_cluster]['clip_windows'].append(window)\n",
        "    else:\n",
        "      liquid_videos_data[id]['cluster_data'][window_cluster]['clip_windows'] = [window]\n",
        "  for cluster in liquid_videos_data[id]['cluster_data']:\n",
        "    liquid_videos_data[id]['cluster_data'][cluster]['clip_windows'] = reduce_cluster_clip_windows(liquid_videos_data[id]['cluster_data'][cluster]['clip_windows'])"
      ],
      "metadata": {
        "id": "kh42LxWJrQOa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cluster_clips(liquid_videos_data, id, video_path, output_path):\n",
        "  #Create path for clips\n",
        "  title = ''.join(c for c in liquid_videos_data[id]['metadata']['title'] if c.isalpha())\n",
        "  clips_ouput_path = f\"{output_path}/{id}_{title}\"\n",
        "  if not os.path.exists(clips_ouput_path):\n",
        "    os.makedirs(clips_ouput_path)\n",
        "\n",
        "  #Create clips\n",
        "  for clustor in liquid_videos_data[id]['cluster_data']:\n",
        "    print(liquid_videos_data[id]['cluster_data'][clustor]['clip_windows'])\n",
        "    for clip_window in liquid_videos_data[id]['cluster_data'][clustor]['clip_windows']:\n",
        "      topic_name = ''.join(c for c in liquid_videos_data[id]['cluster_data'][clustor]['topic'] if c.isalpha())\n",
        "      output_clip_path = f\"{clips_ouput_path}/{clustor}_{topic_name}_{uuid.uuid1()}.mp4\"\n",
        "      if not exists(output_clip_path):\n",
        "        ffmpeg_extract_subclip(video_path, clip_window[0], clip_window[1], targetname=output_clip_path)"
      ],
      "metadata": {
        "id": "d7Gihsf47Et0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Backend Pipleine\n",
        "\n",
        "The following method takes in a list of Youtube video URLs, runs the entire Liquid News Backed V1.0 pipeline on them, and stores the clustering it finds in your drive."
      ],
      "metadata": {
        "id": "uRff83bDV8C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_liquid_news_pipeline(video_list, run_name):\n",
        "  liquid_videos_data = {}\n",
        "  for video_url in video_list:\n",
        "    # Generate Path for Run in LiquidNews Folder\n",
        "    os.mkdir(f'/content/drive/MyDrive/LiquidNews/{run_name}')    \n",
        "\n",
        "    # Generate Path for video storage and audio storage in the \"../LiquidNews/Path\" folder\n",
        "    os.mkdir(f'/content/drive/MyDrive/LiquidNews/{run_name}/videos')    \n",
        "    os.mkdir(f'/content/drive/MyDrive/LiquidNews/{run_name}/audios')  \n",
        "    os.mkdir(f'/content/drive/MyDrive/LiquidNews/{run_name}/transcriptions')  \n",
        "    os.mkdir(f'/content/drive/MyDrive/LiquidNews/{run_name}/embeddings')\n",
        "    os.mkdir(f'/content/drive/MyDrive/LiquidNews/{run_name}/clustor_clips') \n",
        "\n",
        "    # Download the video, and audio and retrieve the metadata\n",
        "    download_video(video_url, f'/content/drive/MyDrive/LiquidNews/{run_name}/videos')\n",
        "    download_video_audio(video_url, f'/content/drive/MyDrive/LiquidNews/{run_name}/audios')\n",
        "    title, id = get_video_information(video_url)\n",
        "\n",
        "    # Initialize children dictionaries & store metadata\n",
        "    liquid_videos_data[id] = {}\n",
        "    liquid_videos_data[id][\"metadata\"] = {}\n",
        "    liquid_videos_data[id][\"caption_token_data\"] = {}\n",
        "    liquid_videos_data[id][\"metadata\"][\"title\"] = title\n",
        "\n",
        "    # Transcribe audio and store it as SRT file in the drive\n",
        "    transcription = audio_to_caption(f'/content/drive/MyDrive/LiquidNews/{run_name}/audios/{id}.mp3', \"\", whisper_model)\n",
        "    with open(f'/content/drive/MyDrive/LiquidNews/{run_name}/transcriptions/{id}.srt', \"w\", encoding=\"utf-8\") as srt:\n",
        "        write_srt(transcription[\"segments\"], file=srt)\n",
        "\n",
        "    # Tokenize the transcription and store it in the liquid_videos_data dictionary.\n",
        "    # (Note we use a hard-coded token size of 4 or V1.0. We plan to us ML to identify\n",
        "    # the optimal one in future versions).\n",
        "    # MODIFIES THE EXISTING liquid_videos_data VARIABLE\n",
        "    caption_tokenizer(transcription[\"segments\"], 2, id, liquid_videos_data)\n",
        "\n",
        "    # Gets embedding for each token and downloads embeddings to drive for the future.\n",
        "    # MODIFIES THE EXISTING liquid_videos_data VARIABLE\n",
        "    embeddings = get_token_embedding(liquid_videos_data, id, 2, \n",
        "                                     f'/content/drive/MyDrive/LiquidNews/{run_name}/embeddings')\n",
        "\n",
        "    # Clusters the embeddings and updates their respective clustering in the liquid_videos_data dict\n",
        "    # MODIFIES THE EXISTING liquid_videos_data VARIABLE\n",
        "    get_clusterings(liquid_videos_data, id,  2)\n",
        "\n",
        "    # Gets the associated topic title (string) for each cluster and stores it in the liquid_videos_data dict\n",
        "    cluster_data = get_cluster_topic(liquid_videos_data, id)\n",
        "    liquid_videos_data[id]['cluster_data'] = cluster_data\n",
        "\n",
        "    # Identifies how to segment videos and then creates clips for each segment\n",
        "    get_cluster_clips_windows(liquid_videos_data, id)\n",
        "    create_cluster_clips(liquid_videos_data, id, \n",
        "                         f'/content/drive/MyDrive/LiquidNews/{run_name}/videos/{id}.mp4', \n",
        "                         f'/content/drive/MyDrive/LiquidNews/{run_name}/clustor_clips')\n",
        "\n",
        "    # Download the final liquid_videos_data dict for future use\n",
        "    dump_path = f'/content/drive/MyDrive/LiquidNews/{run_name}/liquid_videos_data.pkl'\n",
        "    afile = open(dump_path, 'wb')\n",
        "    pickle.dump(liquid_videos_data, afile)\n",
        "    afile.close()\n",
        "\n",
        "run_liquid_news_pipeline(['https://www.youtube.com/watch?v=HB2mGo_0tgA&ab_channel=ABCNews'], \"apple\")"
      ],
      "metadata": {
        "id": "P2HfrhIdWNii"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}